{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 : Intro to TensorFlow and Music Generation with RNNs\n",
    "\n",
    "# Part 2: Music Generation with RNNs\n",
    "\n",
    "이 실습에서는 음악 생성을위한 RNN (Recurrent Neural Network)을 구축하는 방법에 대해 알아볼 것입니다. 우리는 MIDI 음악 툴킷을 사용할 것입니다. 다음 셀을 실행하여 `midi` 패키지가 있는지 확인하십시오. 그러면 MIDI 음악 도구를 Python으로 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Download\n",
    "> https://github.com/louisabraham/python3-midi\n",
    "\n",
    "Installation\n",
    "> python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dependencies and Dataset\n",
    "\n",
    "이 실습에 필요한 관련 패키지와 데이터는 다음과 같습니다. 실습 편의를 위해서 데이터 세트 정리와 생성을 코드로 작성했습니다. 세부적인 작업 내용을 보려면 `create_dataset.py`와 `midi_manipulation.py` 파일을 확인하시기 바랍니다.\n",
    "\n",
    "### 2.1.1 Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "\n",
    "from util.util import print_progress\n",
    "from util.create_dataset import create_dataset, get_batch\n",
    "from util.midi_manipulation import noteStateMatrixToMidi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 The Dataset\n",
    "\n",
    "이 Lab의 데이터 세트는 `lab1`의 `data/` 폴더에서 가져옵니다. 다운로드 한 데이터 세트는 일련의 팝송 발췌 모음입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 songs processed\n",
      "15 songs discarded\n"
     ]
    }
   ],
   "source": [
    "min_song_length  = 128\n",
    "encoded_songs    = create_dataset(min_song_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 세트는 각 노래가 저장된 `np.array`의 리스트 입니다.\n",
    "\n",
    "각 노래는 `(song_length, num_possible_notes)`, `song_length> = min_song_length` 차원으로 되어있어야 합니다.\n",
    "\n",
    "노래의 각 음표의 특징 벡터는 [one-hot](https://en.wikipedia.org/wiki/One-hot) 인코딩으로 처리됩니다. 즉, 바이너리 벡터는 하나의 엔트리만 '1' 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 total songs to learn from\n",
      "(129, 78)\n"
     ]
    }
   ],
   "source": [
    "NUM_SONGS = len(encoded_songs)\n",
    "print(str(NUM_SONGS) + \" total songs to learn from\")\n",
    "print(encoded_songs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 The Recurrent Neural Network (RNN) Model\n",
    "\n",
    "이제 음악 데이터 세트에서 RNN 모델을 정의하고 훈련한 다음 해당 훈련 모델을 사용하여 새 노래를 생성합니다. RNN을 우리의 데이터 세트에 있는 노래 스니펫의 배치로 사용하여 훈련시킬 것입니다.\n",
    "\n",
    "이 모델은 하나의 LSTM 셀을 기반으로 하며, 상태 벡터는 연속적인 음표 간의 시간 종속성을 유지하는 데 사용됩니다.\n",
    "\n",
    "각 시간 단계에서 이전 음표의 시퀀스를 피드합니다. LSTM의 최종 출력 (즉, 마지막 유닛의)은 완전히 연결된 단일 레이어에 공급되어 다음 음에 대한 확률 분포를 출력한다.\n",
    "\n",
    "이 방법으로, 우리는 확률 분포\n",
    "\n",
    "$$ P(x_t\\vert x_{t-L},\\cdots,x_{t-1})$$ \n",
    "\n",
    "여기서 $x_t$는 timestep $t$에서 재생된 음표의 one-hot 인코딩이며 $L$은 아래 그림과 같이 노래 스니펫의 길이입니다.\n",
    "\n",
    "<img src=\"img/lab1ngram.png\" alt=\"Drawing\" style=\"width: 50em;\"/>\n",
    " \n",
    "\n",
    "### 2.2.1 Neural Network Parameters\n",
    "여기서는 모델에 대한 관련 매개 변수를 정의합니다.\n",
    "\n",
    "* `input_size` 와 `output_size`는 각 타임 스텝에서 인코딩된 입력과 출력의 모양이 일치하도록 정의됩니다. 각 노래의 인코딩 된 표현에는 shape (song_length, num_possible_notes)가 있으며 각 타임 스텝에서 연주된 음은 가능한 모든 음에 대해 바이너리 벡터로 인코딩 됩니다. 매개 변수인 input_size와 output_size는 이 벡터 인코딩의 길이 (가능한 음표수)를 반영합니다.\n",
    "\n",
    "* `hidden_size`는 LSTM의 상태 수와 LSTM 이후의 hidden 레이어 크기입니다.\n",
    "* 모델의 `learning_rate`는 1e-4와 0.1 사이 여야합니다.\n",
    "* `training_steps`는 사용할 배치 수입니다.\n",
    "* `batch_size`는 배치에서 사용하는 노래 스니펫 수 입니다.\n",
    "* 모델을 훈련시키기 위해 각 노래에서 `timesteps` 길이의 스니펫을 선택합니다. 이렇게하면 모든 노래 스니펫의 길이가 같아지고 훈련 속도가 빨라집니다.\n",
    "\n",
    "**다른 하이퍼 매개 변수를 사용하여 어떤 것이 가장 효과적인지 확인하십시오.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neural Network Parameters\n",
    "input_size       = encoded_songs[0].shape[1]   # The number of possible MIDI Notes\n",
    "output_size      = input_size                  # Same as input size\n",
    "hidden_size      = 128                         # Number of neurons in hidden layer\n",
    "\n",
    "learning_rate    = 0.001 # Learning rate of the model\n",
    "training_steps   = 200  # Number of batches during training\n",
    "#training_steps   = 0\n",
    "batch_size       = 256    # Number of songs per batch\n",
    "timesteps        = 64    # Length of song snippet -- this is what is fed into the model\n",
    "\n",
    "assert timesteps < min_song_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리 모델을 만들 것입니다. 모델을 만들기 전에 일부 placeholders를 정의하고 가중치를 초기화해야 합니다.\n",
    "\n",
    "먼저 입력과 출력 텐서의 `output_vec`에 대해 `tf.placeholder` 변수 (`dtype`으로 `float`을 사용)를 정의합니다.\n",
    "\n",
    "\n",
    "* 입력 텐서는 훈련 중에 사용된 노래 스니펫 배치를 유지하는 데 사용됩니다. dimensions은 3 차원입니다.\n",
    "  1. 배치의 크기 (임의의 batch size를 처리 할 수 있도록 `None` 사용)\n",
    "  2. 노래 스니펫에서 time steps의 수\n",
    "  3. 가능한 MIDI 음표 수\n",
    "\n",
    "\n",
    "* 출력 텐서는 훈련 배치의 각 노래 스니펫에 대해 입력 텐서에 제공된 노래 스니펫 바로 뒤에 있는 단일 음표를 유지하는 데 사용됩니다. 따라서 dimensions은 2 차원입니다.\n",
    "\n",
    "  1. 배치의 크기 (입력된 텐서와 같이 `None`)\n",
    "  2. 가능한 MIDI 음표 수\n",
    "\n",
    "다음으로 LSTM 후에 완전히 연결된 레이어의 가중치와 바이어스를 초기화해야 합니다. 이러한 가중치와 바이어스을 정의하는 규칙은 딕셔너리를 사용하므로 각 레이어의 이름을 지정할 수 있습니다. LSTM 다음에 하나의 레이어만 있으므로 두 개의 개별 `tf.Variables`로 가중치와 바이어스를 정의 할 수 있습니다. `tf.random_normal`을 사용하여 정규 분포에서 샘플링하여 초기화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_placeholder_shape = [None, timesteps, input_size] #TODO\n",
    "output_placeholder_shape = [None, output_size] #TODO\n",
    "\n",
    "input_vec  = tf.placeholder(\"float\", input_placeholder_shape)  \n",
    "output_vec = tf.placeholder(\"float\", output_placeholder_shape)  \n",
    "\n",
    "# Define weights\n",
    "weights = tf.Variable(tf.random_normal([hidden_size, output_size])) \n",
    "\n",
    "biases = tf.Variable(tf.random_normal([output_size])) # TODO define biases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 RNN Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 파라미터, 입력과 출력 텐서의 placeholder 변수와 초기화된 가중치를 정의했으므로 TensorFlow 계산 그래프 자체를 작성해야 합니다. 우리는 함수 `RNN(input_vec, weights, biases)`을 정의 할 수 있습니다. 이 함수는 placeholder와 변수를 입력 매개 변수로 받고, 그래프를 반환합니다. TensorFlow `rnn` 모듈도 가져왔습니다. [`rnn.BasicLTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) 함수가 유용합니다. 아래 코드를 통해 RNN을 정의해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(input_vec, weights, biases):\n",
    "    \"\"\"\n",
    "    @param input_vec: (tf.placeholder) The input vector's placeholder\n",
    "    @param weights: (tf.Variable) The weights variable\n",
    "    @param biases: (tf.Variable) The bias variable\n",
    "    @return: The RNN graph that will take in a tensor list of shape (batch_size, timesteps, input_size)\n",
    "    and output tensors of shape (batch_size, output_size)\n",
    "    \"\"\"\n",
    "    # First, use tf.unstack() to unstack the timesteps into (batch_size, n_input). \n",
    "    # Since we are unstacking the timesteps axis, we want to pass in 1 as the \n",
    "    #  axis argument and timesteps as the length argument\n",
    "    input_vec = tf.unstack(input_vec, timesteps, 1)\n",
    "\n",
    "    '''TODO: Use TensorFlow's rnn module to define a BasicLSTMCell. \n",
    "    Think about the dimensionality of the output state -- how many hidden states will the LSTM cell have?''' \n",
    "    lstm_cell = rnn.BasicLSTMCell(hidden_size) # TODO \n",
    "\n",
    "    # Now, we want to get the outputs and states from the LSTM cell.\n",
    "    # We rnn's static_rnn function, as described here: \n",
    "    #  https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, input_vec, dtype=tf.float32)\n",
    "    \n",
    "    # Next, let's compute the hidden layer's transformation of the final output of the LSTM.\n",
    "    # We can think of this as the output of our RNN, or as the activations of the final layer. \n",
    "    # Recall that this is just a linear operation: xW + b, where W is the set of weights and b the biases.\n",
    "    '''TODO: Use TensorFlow operations to compute the hidden layer transformation of the final output of the LSTM'''\n",
    "    recurrent_net = tf.matmul(outputs[-1], weights) + biases # TODO \n",
    "    \n",
    "    # Lastly, we want to predict the next note, so we can use this later to generate a song. \n",
    "    # To do this, we generate a probability distribution over the possible notes, \n",
    "    #  by computing the softmax of the transformed final output of the LSTM.\n",
    "    '''TODO: Use the TensorFlow softmax function to output a probability distribution over possible notes.'''\n",
    "    prediction = tf.nn.softmax(recurrent_net) # TODO\n",
    "    \n",
    "    # All that's left is to return recurrent_net (the RNN output) and prediction (the softmax output)\n",
    "    return recurrent_net, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Loss, Training, and Accuracy Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 여전히 네트워크에 대해 몇 가지 사항을 정의해야 합니다. 계산 그래프의 본문을 정의했지만 loss 연산, training 연산과 accuracy 함수가 필요합니다.\n",
    "\n",
    "* Loss : 우리는 평균 [softmax cross entropy loss](https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2)을 사용하여 RNN의 다음 음표에 대한 예측과 실제 다음 음표 간의 확률 에러를 측정합니다.\n",
    "* Training : loss 연산을 정의한 후에는 손실을 최소화하기 위해 최적화 도구를 사용합니다.\n",
    "* Accuracy : 네트워크에서 예측한 가장 가능성이 높은 다음 음표와 실제 다음 음표를 비교하여 정확성을 측정합니다. \n",
    "\n",
    "이제 나머지 구성 요소를 정의 할 수 있습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable rnn/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-6-e671d5136e8d>\", line 21, in RNN\n    outputs, states = rnn.static_rnn(lstm_cell, input_vec, dtype=tf.float32)\n  File \"<ipython-input-7-685370dd760d>\", line 1, in <module>\n    logits, prediction = RNN(input_vec, weights, biases)\n  File \"/Users/jiho/.virtualenvs/gurus/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-685370dd760d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-e671d5136e8d>\u001b[0m in \u001b[0;36mRNN\u001b[0;34m(input_vec, weights, biases)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# We rnn's static_rnn function, as described here:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#  https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Next, let's compute the hidden layer's transformation of the final output of the LSTM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mstatic_rnn\u001b[0;34m(cell, inputs, initial_state, dtype, sequence_length, scope)\u001b[0m\n\u001b[1;32m   1322\u001b[0m             state_size=cell.state_size)\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m       \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[0mvarscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0;31m# pylint: disable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m       \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1312\u001b[0m       \u001b[0;31m# pylint: enable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;31m# method.  See the class docstring for more details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\n\u001b[0;32m--> 339\u001b[0;31m                                      *args, **kwargs)\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0minput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m           \u001b[0;31m# Note: not all sub-classes of Layer call Layer.__init__ (especially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, inputs_shape)\u001b[0m\n\u001b[1;32m    586\u001b[0m     self._kernel = self.add_variable(\n\u001b[1;32m    587\u001b[0m         \u001b[0m_WEIGHTS_VARIABLE_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         shape=[input_depth + h_depth, 4 * self._num_units])\n\u001b[0m\u001b[1;32m    589\u001b[0m     self._bias = self.add_variable(\n\u001b[1;32m    590\u001b[0m         \u001b[0m_BIAS_VARIABLE_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36madd_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             partitioner=partitioner)\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minit_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus/lib/python3.6/site-packages/tensorflow/python/training/checkpointable.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    434\u001b[0m     new_variable = getter(\n\u001b[1;32m    435\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1318\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1319\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/.virtualenvs/gurus/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/.virtualenvs/gurus/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/.virtualenvs/gurus/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gurus/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    731\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 733\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    734\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable rnn/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-6-e671d5136e8d>\", line 21, in RNN\n    outputs, states = rnn.static_rnn(lstm_cell, input_vec, dtype=tf.float32)\n  File \"<ipython-input-7-685370dd760d>\", line 1, in <module>\n    logits, prediction = RNN(input_vec, weights, biases)\n  File \"/Users/jiho/.virtualenvs/gurus/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "logits, prediction = RNN(input_vec, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS OPERATION:\n",
    "'''TODO: Use TensorFlow to define the loss operation as the mean softmax cross entropy loss. \n",
    "TensorFlow has built-in functions for you to use. '''\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=logits, labels=output_vec))  # TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING OPERATION:\n",
    "'''TODO: Define an optimizer for the training operation. \n",
    "Remember we have already set the `learning_rate` parameter.'''\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) # TODO\n",
    "train_op = optimizer.minimize(loss_op) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCURACY: We compute the accuracy in two steps.\n",
    "\n",
    "# First, we need to determine the predicted next note and the true next note, across the training batch, \n",
    "#  and then determine whether our prediction was correct. \n",
    "# Recall that we defined the placeholder output_vec to contain the true next notes for each song snippet in the batch.\n",
    "'''TODO: Write an expression to obtain the index for the most likely next note predicted by the RNN.'''\n",
    "true_note = tf.argmax(output_vec,1)\n",
    "pred_note = tf.argmax(prediction, 1) # TODO\n",
    "correct_pred = tf.equal(pred_note, true_note)\n",
    "\n",
    "# Next, we obtain a value for the accuracy. \n",
    "# We cast the values in correct_pred to floats, and use tf.reduce_mean\n",
    "#  to figure out the fraction of these values that are 1's (1 = correct, 0 = incorrect)\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZER:\n",
    "# Finally we create an initializer to initialize the variables we defined in Section 2.2.2\n",
    "# We use TensorFlow's global_variables_initializer\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Training the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리의 RNN 모델을 훈련 할 준비가 되었습니다! [`tf.InteractiveSession()`](https://www.tensorflow.org/api_docs/python/tf/InteractiveSession)을 사용하여 그래프를 실행하고 모델을 훈련합니다.\n",
    "\n",
    "우리는 다음을 수행해야 합니다.\n",
    "\n",
    "0. 세션 시작\n",
    "1. 변수 초기화\n",
    "2. 각 훈련 단계 :\n",
    "  * 교육 배치 가져 오기 :\n",
    "    * `batch_x` : 노래 스니펫 배치\n",
    "    * `batch_y` : 배치에서 각 노래 스니펫에 대한 다음 음표\n",
    "  * 배치를 통해 training 연산 실행\n",
    "  * display step : \n",
    "    * 손실과 정확도를 계산하고 이러한 측정 항목을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1) Launch the session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# 2) Initialize the variables\n",
    "sess.run(init)\n",
    "\n",
    "# 3) Train!\n",
    "display_step = 10 # how often to display progress\n",
    "for step in range(training_steps):\n",
    "    # GET BATCH\n",
    "    # Add the line to get training data batch (see util.get_batch or whatever for args) FIX THIS !\n",
    "    '''TODO: Fill in the function call to obtain a training data batch. \n",
    "    Hint: See the file util/create_dataset.py.'''\n",
    "    batch_x, batch_y = get_batch(encoded_songs, batch_size, timesteps, input_size, output_size) # TODO\n",
    "    \n",
    "    # TRAINING: run the training operation with a feed_dict to fill in the placeholders\n",
    "    '''TODO: Feed the training batch into the feed_dict.'''\n",
    "    feed_dict = {\n",
    "                    input_vec: batch_x, # TODO remove after colon\n",
    "                    output_vec: batch_y # TODO remove after colon\n",
    "                }\n",
    "    sess.run(train_op, feed_dict=feed_dict)\n",
    "    \n",
    "    # DISPLAY METRICS\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        # LOSS, ACCURACY: Compute the loss and accuracy by running both operations \n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict=feed_dict)     \n",
    "        suffix = \"\\nStep \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                 \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                 \"{:.3f}\".format(acc)\n",
    "\n",
    "        print_progress(step, training_steps, barLength=50, suffix=suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "음악을 생성하기 위한 정확한 정확도 수준은 무엇입니까? 100%의 정확도는 모델이 데이터 세트의 모든 노래를 암기했으며 마음대로 재생할 수 있음을 의미합니다. 0%의 정확도는 랜덤 노이즈를 의미합니다. \n",
    "\n",
    "Gary Marcus의 말에 따르면 \"순수하게 예측 가능하거나 완전히 예측할 수없는 음악은 일반적으로 불쾌한 것으로 간주됩니다. 지나치게 예상 범위 내에만 있으면 지루하고, 너무 예측할 수 없으면 귀에 거슬리게 됩니다.\"\n",
    "\n",
    "경험적으로 `75%`와 `90%` 사이에서 좋은 결과를 얻었지만, 생성된 결과물을 직접 들어보고 직접 확인해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Music Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리는 훈련된 RNN 모델을 사용하여 음악을 만들 수 있습니다! 음악을 생성할 때 우리는 시작하기 위해 모델에 일종의 씨앗(feed)을 공급해야 합니다 (시작해야 할 것이 없으면 어떤 음표도 예측할 수 없기 때문에!).\n",
    "\n",
    "생성된 시드가 있으면 우리는 훈련된 RNN을 사용하여 각 연속된 음표를 반복적으로 예측할 수 있습니다. 보다 구체적으로 말하면, RNN이 연속된 음표에 대한 확률 분포를 출력한다는 것을 상기하십시오. 추론을 위해, 우리는 이러한 분포로부터 반복적으로 샘플을 추출하고 샘플을 사용하여 생성된 노래를 인코딩합니다.\n",
    "\n",
    "그런 다음 파일에 기록하여 듣기만하면 됩니다.\n",
    "\n",
    "`/generated` 폴더에는 3 개의 예제 생성 파일이 있습니다. 이 예제를 생성하기 위해 다음 매개 변수를 사용하여 모델을 훈련했습니다. `hidden_size = 256`, `learning_rate = 0.001`, `training_steps = 2000`, `batch_size = 128`. 다른 매개 변수를 사용하여 더 나은 사운드 모델을 훈련 시키십시오!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_SEED_RANDOMLY = False # Use a random snippet as a seed for generating the new song.\n",
    "if GEN_SEED_RANDOMLY:\n",
    "    ind = np.random.randint(NUM_SONGS)\n",
    "else:\n",
    "    ind = 41 # \"How Deep is Your Love\" by Calvin Harris as a starting seed\n",
    "    \n",
    "gen_song = encoded_songs[ind][:timesteps].tolist() # TODO explore different (non-random) seed options\n",
    "    \n",
    "# generate music!\n",
    "for i in range(500):\n",
    "    seed = np.array([gen_song[-timesteps:]])\n",
    "    # Use our RNN for prediction using our seed! \n",
    "    '''TODO: Write an expression to use the RNN to get the probability for the next note played based on the seed.\n",
    "    Remember that we are now using the RNN for prediction, not training.'''\n",
    "    predict_probs = sess.run(prediction, feed_dict = {input_vec:seed}) # TODO\n",
    "\n",
    "    # Define output vector for our generated song by sampling from our predicted probability distribution\n",
    "    played_notes = np.zeros(output_size)\n",
    "    '''TODO: Sample from the predicted distribution to determine which note gets played next.\n",
    "    You can use a function from the numpy.random library to do this.\n",
    "    Hint 1: range(x) produces a list of all the numbers from 0 to x\n",
    "    Hint 2: make sure predict_probs has the \"shape\" you expect. you may need to flatten it: predict_probs.flatten()'''\n",
    "    #print(np.argmax(predict_probs[0]))\n",
    "    plt.plot(predict_probs[0])\n",
    "    sampled_note = np.random.choice(range(output_size), p=predict_probs[0]) # TODO\n",
    "    #sampled_note = np.argmax(predict_probs[0])\n",
    "    played_notes[sampled_note] = 1\n",
    "    gen_song.append(played_notes)\n",
    "\n",
    "noteStateMatrixToMidi(gen_song, name=\"generated/gen_song_0\")\n",
    "noteStateMatrixToMidi(encoded_songs[ind], name=\"generated/base_song_0\")\n",
    "print(\"saved generated song! seed ind: {}\".format(ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 What if We Didn't Train? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련의 영향을 이해하고 네트워크의 진행 상태를 확인하려면 훈련받지 않은 모델로 음악이 어떻게 들리는지 확인하십시오.\n",
    "\n",
    "이렇게 하려면 `training_steps = 0`으로 설정하고 위의 학습 셀을 다시 실행한 다음 추측 파이프 라인을 다시 실행하여 훈련을 받지 않은 모델로 노래를 생성하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gurus",
   "language": "python",
   "name": "gurus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
