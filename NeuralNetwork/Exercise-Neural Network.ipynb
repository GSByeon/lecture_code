{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network 실습\n",
    "\n",
    "## 사용할 데이터\n",
    "\n",
    "1. Car(ToyotaCorolla) - regression 예제\n",
    "2. wdbc - classification 예제\n",
    "\n",
    "### 실습 모듈 import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize,StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier,MLPRegressor\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류 성능 지표 함수 정의 \n",
    "1. Confusion matrix\n",
    "2. Accuracay\n",
    "3. Recall\n",
    "4. Precision\n",
    "5. F-1 measure\n",
    "6. Balanced measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# 분류기 성능 지표 산출 함수\n",
    "##########################################################################\n",
    "def evaluation(y_true, y_pred):\n",
    "    cfm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "    acc = (cfm[0, 0] + cfm[1, 1]) / np.sum(cfm)\n",
    "    recall = cfm[1, 1] / np.sum(cfm[:, 1])\n",
    "    precision = cfm[1, 1] / np.sum(cfm[1, :])\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    bcr = np.sqrt(cfm[1, 1] / (sum(cfm[1, :])) * cfm[0, 0] / (sum(cfm[0, :])))\n",
    "    return [cfm, acc, recall, precision, f1, bcr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 Neural Network model 함수 정의\n",
    "\n",
    "**sklearn.neural_network.MLPClassifier** : 'http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# Neural network model\n",
    "##########################################################################\n",
    "def NN_classifier(train_x, train_y, hidden_nodes, plot=False):\n",
    "    # NN 객체 생성\n",
    "    NN_clf = MLPClassifier(solver='lbfgs', random_state=0, \n",
    "                           hidden_layer_sizes=[hidden_nodes], verbose=True)\n",
    "    # NN fit\n",
    "    NN_clf.fit(X=train_x, y=train_y)\n",
    "\n",
    "    if plot:\n",
    "       \n",
    "        mglearn.plots.plot_2d_separator(NN_clf, train_x, fill=True, alpha=.3)\n",
    "        mglearn.discrete_scatter(train_x[:, 0], train_x[:, 1], train_y)\n",
    "        plt.xlabel(\"특성 0\")\n",
    "        plt.ylabel(\"특성 1\")\n",
    "        plt.show()\n",
    "\n",
    "    return NN_clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Neural Network Activation function ###\n",
    "line = np.linspace(-3, 3, 100)\n",
    "plt.plot(line, np.tanh(line), label=\"tanh\")\n",
    "plt.plot(line, np.maximum(line, 0), linestyle='--', label=\"relu\")\n",
    "plt.plot(line,1 / (1 + np.exp(-line)),linestyle='-.',label='sigmoid')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"relu(x), tanh(x), sigmoid(x)\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 데이터를 활용한 비선형 분류 모델의 역활 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Hidden node 수에 따른 비선형 분류모델의 역활 수행\n",
    "# 비선형 분류 데이터 생성\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 히든노드수가 2개인경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2개의 히든노드\n",
    "mlp_2 = MLPClassifier(solver='lbfgs', random_state=0,  # default relu\n",
    "                      hidden_layer_sizes=[2],verbose=True)\n",
    "mlp_2.fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp_2, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Attribute 0\")\n",
    "plt.ylabel(\"Attribute 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 히든노드수가 10개인경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 10개의 히든노드\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=42)\n",
    "mlp_10 = MLPClassifier(solver='lbfgs', random_state=0, \n",
    "                       hidden_layer_sizes=[10],verbose=True)\n",
    "mlp_10.fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp_10, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Attribute 0\")\n",
    "plt.ylabel(\"Attribute 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10개의 히든노드로 구성된 2개의 은닉층을 구성한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 10개의 유닛으로 된 두 개의 은닉층\n",
    "mlp = MLPClassifier(solver='lbfgs', random_state=0,\n",
    "                    hidden_layer_sizes=[10, 10])\n",
    "mlp.fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Attribute 0\")\n",
    "plt.ylabel(\"Attribute 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actication 함수를 'tanh'로 바꿨을 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tanh 활성화 함수가 적용된 10개의 유닛으로 된 두 개의 은닉층\n",
    "mlp = MLPClassifier(solver='lbfgs', activation='tanh',\n",
    "                    random_state=0, hidden_layer_sizes=[10, 10])\n",
    "mlp.fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Attribute 0\")\n",
    "plt.ylabel(\"Attribute 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actication 함수를 'sigmoid'로 바꿨을 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tanh 활성화 함수가 적용된 10개의 유닛으로 된 두 개의 은닉층\n",
    "mlp = MLPClassifier(solver='lbfgs', activation='logistic', # sigmoid\n",
    "                    random_state=0, hidden_layer_sizes=[10, 10])\n",
    "mlp.fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Attribute 0\")\n",
    "plt.ylabel(\"Attribute 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLPClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network의 학습 iteration에 따른 분류 결정경계면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Max iteration에 따른 분류 결정경계면\n",
    "fig, axes = plt.subplots(1, 6, figsize=(24,4))\n",
    "for ax, n_maxiter in zip(axes, [10,20,40,50,100,200]):\n",
    "    mlp = MLPClassifier(solver='lbfgs', random_state=0,\n",
    "                        hidden_layer_sizes=10,max_iter=n_maxiter)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\n",
    "    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\n",
    "    ax.set_title(\"n_hidden=[{}]\\nmax_iteration={}\".format(\n",
    "                  5, n_maxiter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network의 hidden node의 수와 regularize 파라미터 alpha의 변화에 따른 분류 경계면\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hidden node수에 따른 인공신경망 결정경계면\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 20))\n",
    "for axx, n_hidden_nodes in zip(axes, [2,4,8,10]):\n",
    "    for ax, alpha in zip(axx, [0.0001, 0.01, 0.1, 0.5]):\n",
    "        mlp = MLPClassifier(solver='lbfgs', random_state=0,\n",
    "                            hidden_layer_sizes=[n_hidden_nodes],\n",
    "                            alpha=alpha)\n",
    "        mlp.fit(X_train, y_train)\n",
    "        mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\n",
    "        mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\n",
    "        ax.set_title(\"n_hidden=[{}]\\nalpha={:.4f}\".format(\n",
    "                      n_hidden_nodes, alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network의 hidden node의 수와 regularize 파라미터 alpha의 변화에 따른 분류 경계면 (2개의 은닉층)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2개의 Hidden Layer Hidden node수에 따른 인공신경망 결정경계면\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 20))\n",
    "for axx, n_hidden_nodes in zip(axes, [2,4,6,8]):\n",
    "    for ax, alpha in zip(axx, [0.0001, 0.01, 0.1, 0.5]):\n",
    "        mlp = MLPClassifier(solver='lbfgs', random_state=0,\n",
    "                            hidden_layer_sizes=[n_hidden_nodes, n_hidden_nodes],\n",
    "                            alpha=alpha)\n",
    "        mlp.fit(X_train, y_train)\n",
    "        mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\n",
    "        mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\n",
    "        ax.set_title(\"n_hidden=[{}, {}]\\nalpha={:.4f}\".format(\n",
    "            n_hidden_nodes, n_hidden_nodes, alpha))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network의 regularize 파라미터 alpha의 변화에 따른 다양한 데이터의 분류 경계면 시각화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### 데이터 형태에 따른 L2 penalty(alpha) 변화에 따른 분류결정 경계면 Example\n",
    "h = .02  # step size in the mesh\n",
    "alphas = np.logspace(-5, 3, 5)\n",
    "names = []\n",
    "for i in alphas:\n",
    "    names.append('alpha ' + str(i))\n",
    "\n",
    "classifiers = []\n",
    "for i in alphas:\n",
    "    classifiers.append(MLPClassifier(hidden_layer_sizes=(100, ), \n",
    "                                     alpha=i, random_state=1))\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=0, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable]\n",
    "\n",
    "figure = plt.figure(figsize=(17, 9))\n",
    "i = 1\n",
    "\n",
    "# iterate over datasets\n",
    "for X, y in datasets:\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "    # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, \n",
    "               cmap=cm_bright, alpha=0.6)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, \n",
    "        # we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot also the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='black', s=25)\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   alpha=0.6, edgecolors='black', s=25)\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "figure.subplots_adjust(left=.02, right=.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실제 분류 데이터를 활용하여 Nerual Network 학습 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 실제 데이터를 활용한 Neural network 학습\n",
    "# classification\n",
    "wdbc = pd.read_csv('./data/wdbc.csv', delimiter=',')\n",
    "#print(wdbc)\n",
    "print(wdbc.shape)\n",
    "\n",
    "wdbc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wdbc = np.array(wdbc)\n",
    "\n",
    "# 약 4:6의 label 비율을 가짐\n",
    "wdbc_y = np.array(list(map(lambda x: int(x=='M'), wdbc[:, 30])))\n",
    "print('Label Balance : {0}'.format( round(sum(wdbc_y) / len(wdbc_y),4) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wdbc_x = np.delete(wdbc, [30], axis=1)\n",
    "\n",
    "# data scaling\n",
    "wdbc_x = normalize(wdbc_x, axis=0, norm='max')\n",
    "\n",
    "# training, text data로 나누기\n",
    "wdbc_train_x, wdbc_test_x, wdbc_train_y, wdbc_test_y = \\\n",
    "        train_test_split(wdbc_x, wdbc_y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation을 통한 인공 신경망 학습 파라미터 tuning\n",
    "\n",
    "\n",
    "**sklearn.model_selection.GridSearchCV** : 'http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Cross validation을 통한 최적의 hidden node & max iteration 찾기\n",
    "def NN_CV_classifier(train_x, train_y, max_nodes,CV_N ,plot=False):\n",
    "    # 1부터 max node 수에 대해서 최적의 Hidden node 수를 cross-validation을 이용하여 찾아보자\n",
    "    parameters = {'hidden_layer_sizes': np.arange(start=1, stop=max_nodes,step=10).tolist(),\n",
    "                  'max_iter': [100,200,300,400,500]}\n",
    "    tmp_NN = MLPClassifier()\n",
    "    #cross-validation 결과물을 담을 변수\n",
    "    cv_NN_scores = []\n",
    "    clf = GridSearchCV(tmp_NN, parameters, cv=CV_N)\n",
    "    clf.fit(train_x,train_y)\n",
    "\n",
    "    print( clf.best_params_)\n",
    "    optimal_parameters = list(clf.best_params_.values())\n",
    "\n",
    "    print(\"The optimal number of hidden nodes : {}\\n  & max iteration : {}\".format(optimal_parameters[0],optimal_parameters[1]))\n",
    "\n",
    "    #최종 KNN 회귀모형 적합 및 분류 성능 지표 산출\n",
    "    opt_NN = MLPClassifier(hidden_layer_sizes=optimal_parameters[0],\n",
    "                           max_iter=optimal_parameters[1])\n",
    "    opt_NN.fit(X=train_x, y=train_y)\n",
    "\n",
    "    return opt_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기존 LDA와의 분류 성능비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### LDA  분류기와 성능 비교\n",
    "##########################################################################\n",
    "# LDA 분류 모델\n",
    "##########################################################################\n",
    "def LDA_classifier(train_x, train_y, plot=False):\n",
    "    # LDA 객체 생성\n",
    "    lda_clf = LinearDiscriminantAnalysis()\n",
    "    # LDA fit\n",
    "    lda_clf.fit(X=train_x, y=train_y)\n",
    "\n",
    "    if plot:\n",
    "        # LDA 결과 구해진 w에 대해 사영시키고, histogram 확인\n",
    "        # 사영된 값 추출\n",
    "        transposed_x = lda_clf.transform(X=train_x)\n",
    "        # 각 class 별로 데이터 나누기\n",
    "        transposed_x_c0 = transposed_x[train_y == 0]\n",
    "        transposed_x_c1 = transposed_x[train_y == 1]\n",
    "        # histogram 산출\n",
    "        plt.hist(transposed_x_c0, bins=100, color='b', label='Class 0')\n",
    "        plt.hist(transposed_x_c1, bins=100, color='r', label='Class 1')\n",
    "        plt.title(\"Transposed data Histogram\")\n",
    "        plt.xlabel(\"Value\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return lda_clf\n",
    "\n",
    "# LDA\n",
    "wdbc_lda_classifier = LDA_classifier(train_x=wdbc_train_x, \n",
    "                                     train_y=wdbc_train_y, plot=True)\n",
    "wdbc_lda_pred = wdbc_lda_classifier.predict(X=wdbc_test_x)\n",
    "wdbc_lda_pred_prob = wdbc_lda_classifier.predict_proba(X=wdbc_test_x)\n",
    "wdbc_lda_cfm, wdbc_lda_acc, wdbc_lda_pre, wdbc_lda_rec, wdbc_lda_f1, wdbc_lda_bcr =\\\n",
    "                    evaluation(y_true=wdbc_test_y, y_pred=wdbc_lda_pred)\n",
    "\n",
    "print('==== LDA - Classifier ================')\n",
    "print('accuracy:{}\\nrecall:{}\\nprecision:{}\\nF1:{}\\nBCR:{}'.format(round(wdbc_lda_acc,4), \n",
    "                                                                   round(wdbc_lda_pre,4), \n",
    "                                                                   round(wdbc_lda_rec,4), \n",
    "                                                                   round(wdbc_lda_f1,4), \n",
    "                                                                   round(wdbc_lda_bcr,4)))\n",
    "print('=======================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Neural Network base & Optimized Neural Network\n",
    "wdbc_NN_clssifier = NN_classifier(train_x=wdbc_train_x, train_y=wdbc_train_y,\n",
    "                                  hidden_nodes=10)\n",
    "\n",
    "wdbc_optNN_classifier = NN_CV_classifier(train_x=wdbc_train_x, train_y=wdbc_train_y,\n",
    "                                         max_nodes=100,CV_N=5)\n",
    "\n",
    "wdbc_NN_pred = wdbc_NN_clssifier.predict(X=wdbc_test_x)\n",
    "wdbc_nn_cfm, wdbc_nn_acc, wdbc_nn_pre, wdbc_nn_rec, wdbc_nn_f1, wdbc_nn_bcr =\\\n",
    "                evaluation(y_true=wdbc_test_y, y_pred=wdbc_NN_pred)\n",
    "\n",
    "\n",
    "print('==== NN - Classifier ================')\n",
    "print('accuracy:{}\\nrecall:{}\\nprecision:{}\\nF1:{}\\nBCR:{}'.format(round(wdbc_nn_acc,4), \n",
    "                                                                   round(wdbc_nn_pre,4), \n",
    "                                                                   round(wdbc_nn_rec,4), \n",
    "                                                                   round(wdbc_nn_f1,4), \n",
    "                                                                   round(wdbc_nn_bcr,4)))\n",
    "print('====================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단순 LDA와 Neural Network 분류 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wdbc_optNN_pred = wdbc_optNN_classifier.predict(X=wdbc_test_x)\n",
    "wdbc_onn_cfm, wdbc_onn_acc, wdbc_onn_pre, wdbc_onn_rec, wdbc_onn_f1, wdbc_onn_bcr =\\\n",
    "                evaluation(y_true=wdbc_test_y, y_pred=wdbc_optNN_pred)\n",
    "\n",
    "print('==== NN - Optimized Classifier ================')\n",
    "print('accuracy:{}\\nrecall:{}\\nprecision:{}\\nF1:{}\\nBCR:{}'.format(round(wdbc_onn_acc,4), \n",
    "                                                                   round(wdbc_onn_pre,4), \n",
    "                                                                   round(wdbc_onn_rec,4), \n",
    "                                                                   round(wdbc_onn_f1,4), \n",
    "                                                                   round(wdbc_onn_bcr,4)))\n",
    "print('==============================================')\n",
    "\n",
    "print('')\n",
    "\n",
    "summary_tb = pd.DataFrame({'Model':['LDA','Base Neural Network','Optimized Neural Network'],\n",
    "                           'Accuracy': [round(wdbc_lda_acc, 4), round(wdbc_nn_acc, 4), round(wdbc_onn_acc, 4)],\n",
    "                           'Precision': [round(wdbc_lda_pre, 4), round(wdbc_nn_pre, 4), round(wdbc_onn_pre, 4)],\n",
    "                           'Recall': [round(wdbc_lda_rec, 4), round(wdbc_nn_rec, 4), round(wdbc_onn_rec, 4)],\n",
    "                           'F1-measure': [round(wdbc_lda_f1, 4), round(wdbc_nn_f1, 4), round(wdbc_onn_f1, 4)],\n",
    "                           'BCR': [round(wdbc_lda_bcr, 4), round(wdbc_nn_bcr, 4), round(wdbc_onn_bcr, 4)]})\n",
    "summary_tb = summary_tb[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-measure', 'BCR']]\n",
    "print(summary_tb)\n",
    "\n",
    "summary_tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실제 분류 데이터를 활용하여 Nerual Network 학습 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# Regression\n",
    "##############\n",
    "car = pd.read_csv('./data/ToyotaCorolla.csv', delimiter=',')\n",
    "\n",
    "print(car.shape)\n",
    "car.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "car = np.array(car)\n",
    "car_y = np.array(car[:, 0])\n",
    "car_x = np.delete(car, [0], axis=1)\n",
    "\n",
    "# data scaling\n",
    "car_x = normalize(car_x, axis=0, norm='max')\n",
    "\n",
    "# training, text data로 나누기\n",
    "car_train_x, car_test_x, car_train_y, car_test_y =\\\n",
    "            train_test_split(car_x, car_y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network 학습\n",
    "\n",
    "\n",
    "** sklearn.neural_network.MLPRegressor** :'http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLPRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN_CV_regresser(train_x, train_y, max_nodes,CV_N ,plot=False):\n",
    "    #1부터 10까지의 neighbor에 대해 최적의 k를 cross-validation을 이용하여 찾아보자\n",
    "    parameters = {'hidden_layer_sizes': np.arange(start=1, stop=max_nodes,step=10).tolist(),\n",
    "                  'max_iter': [200,400,800,1000,2000]}\n",
    "    tmp_NN = MLPRegressor( learning_rate_init=0.01 )\n",
    "    \n",
    "    #cross-validation 결과물을 담을 변수\n",
    "    cv_NN_scores = []\n",
    "    clf = GridSearchCV(tmp_NN, parameters,cv=CV_N)\n",
    "    clf.fit(train_x,train_y)\n",
    "\n",
    "    print( clf.best_params_)\n",
    "    optimal_parameters = list(clf.best_params_.values())\n",
    "\n",
    "    print(\"The optimal number of hidden nodes : {}\\n  & max iteration : {}\".format(optimal_parameters[0],\n",
    "                                                                                   optimal_parameters[1]))\n",
    "\n",
    "    #최종 KNN 회귀모형 적합 및 분류 성능 지표 산출\n",
    "    opt_NN = MLPRegressor(hidden_layer_sizes=optimal_parameters[0],\n",
    "                          max_iter=optimal_parameters[1])\n",
    "    opt_NN.fit(X=train_x, y=train_y)\n",
    "\n",
    "    return opt_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_NNR = MLPRegressor()\n",
    "base_NNR.fit(car_train_x, car_train_y)\n",
    "car_base_NNR_pred=base_NNR.predict(car_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training with optimal hidden nodes & max iteration\n",
    "car_optNNR=NN_CV_regresser(train_x=car_train_x, train_y=car_train_y,\n",
    "                           max_nodes=100,CV_N=5)\n",
    "car_optNNR_pred = car_optNNR.predict(X=car_test_x)\n",
    "\n",
    "### base NN regression performance\n",
    "NN_base_regression_rmse = np.sqrt(mean_squared_error(y_true=car_test_y, \n",
    "                                                     y_pred=car_base_NNR_pred))\n",
    "\n",
    "### optimal NN regression performance\n",
    "NN_opt_regression_rmse = np.sqrt(mean_squared_error(y_true=car_test_y, \n",
    "                                                    y_pred=car_optNNR_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network for regression 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Defualt neural network regressor RMSE: {}\".format(round(NN_base_regression_rmse,4)))\n",
    "print(\"Optimal neural network regressor RMSE:: {}\".format(round(NN_opt_regression_rmse,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
